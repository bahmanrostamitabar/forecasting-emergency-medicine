# Results and discussion {#sec-results}

```{r}
#| include: false
library(kableExtra)
rmsse <- readr::read_rds(here::here("results/rmsse.rds"))
mase <- readr::read_rds(here::here("results/mase.rds"))
crps <- readr::read_rds(here::here("results/crps.rds")) |>
  pivot_wider(names_from = model, values_from = crps) |>
  group_by(method, h, series) |>
  mutate(across(where(is.numeric), ~ .x / naiveecdf)) |>
  ungroup()
```

In this section, we compare the forecasting performance of Naive, ETS, GLM, Tscount and the ensemble model using base forecast and Minimum Trace (Mint) methods. We have also computed the forecast accuracy for Ordinary Least Square (OLS) and Weighted Least Square (WLS) approaches. However, they are not reported here because their accuracy is outperformed by Mint. We should also note that forecasts, and consequently their corresponding errors, are generated for the entire hierarchy and they could be reported at any level, if required. 

The overall forecasting performance is reported in @tbl-result, in which the average forecast accuracy per model, method and the hierarchical level is presented. Reported forecast accuracy are averaged across all forecast horizons, rolling origins and series at each level. @tbl-result presents both point and probabilistic forecast accuracy at total, control area, health board and bottom level series. Point forecast performance are reported using MASE and RMSSE in @tbl-result-1  and @tbl-result-2, respectively. Probabilistic forecast accuracy is reported using CRPS in @tbl-result-3. The bold entries in each table identify a combination of method and model that performs best for the corresponding level (i.e. each column), based on the smallest values of accuracy measures.

@tbl-result-1 shows that forecast reconciliation (i.e. Mint) improves forecast accuracy at the higher levels of the hierarchy including total, control area and health board. However, it is does not result in accuracy improvement at the bottom level series, fro which base forecasts are more accurate. This might be due to the noisy structure of time series at the bottom level and the fact that base forecasts produced for the bottom of hierarchy are from well-specified forecasting models and therefore does not gain benefit from forecast reconciliation. Generally, if the bottom-level series are sufficiently smooth, base forecast is expected to outperform hierarchical reconciliation due to the better forecasts available at the level. 
It is also clear from @tbl-result-1 that the ensemble method improves forecast accuracy at total, control area and health board. However, this does not remain valid for bottom series where the Poisson regression by GLM outperforms other approaches.

@tbl-result-2 also reports point forecast accuracy but using RMSSE. The outperformance of forecast reconciliation using Mint at higher levels of hierarchy is also confirmed by RMSSE. However, for the bottom series, we observe that the base forecasts generated by ETS are the most accurate, followed by its reconciliations forecasts. 


```{r}
#| label: tbl-result
#| tbl-cap: "Average forecast perfomance calculated on out-of-sample with time series cross validation applied to attended incident data. The best approach is highlighted in bold."
#| tbl-subcap: 
#|   - "Point forecast accuracy using MASE"
#|   - "Point forecast accuracy using RMSSE"
#|   - "Probabilistic forecast accuracy using CRPS"
#| layout-nrow: 3
rmsse1 <- rmsse |>
  filter(
    method %in% c("mint","base"), model != "qcomb"
  ) |> group_by(method,model,series) |> summarise(rmsse = mean(rmsse), .groups = "drop") |> 
  pivot_wider(names_from = series, values_from = rmsse) |> 
  mutate_if(is.numeric, format, digits=4,nsmall = 0) |> 
  select(method,model,Total,`Control areas`,`Health boards`,Bottom)
mase1 <- mase |>
  filter(
    method %in% c("mint","base"), model != "qcomb"
  ) |> group_by(method,model,series) |> summarise(mase = mean(mase), .groups = "drop") |> 
  pivot_wider(names_from = series, values_from = mase) |> 
  mutate_if(is.numeric, format, digits=4,nsmall = 0) |> 
  select(method,model,Total,`Control areas`,`Health boards`,Bottom)
crps1 <- readr::read_rds(here::here("results/crps.rds")) |>
  filter(method %in% c("mint","base"), model != "qcomb") |> 
  group_by(method,model,series) |> summarise(crps = mean(crps), .groups = "drop") |> 
  pivot_wider(names_from = series, values_from = crps)|> 
  mutate_if(is.numeric, format, digits=4,nsmall = 0) |> 
  select(method,model,Total,`Control areas`,`Health boards`,Bottom)
  

mase1[6, 3] <- cell_spec(mase1[6, 3], "latex", bold = T)
mase1[6, 4] <- cell_spec(mase1[6, 4], "latex", bold = T)
mase1[6, 5] <- cell_spec(mase1[6, 5], "latex", bold = T)
mase1[3, 6] <- cell_spec(mase1[3, 6], "latex", bold = T)

rmsse1[6, 3] <- cell_spec(rmsse1[6, 3], "latex", bold = T)
rmsse1[6, 4] <- cell_spec(rmsse1[6, 4], "latex", bold = T)
rmsse1[6, 5] <- cell_spec(rmsse1[6, 5], "latex", bold = T)
rmsse1[2, 6] <- cell_spec(rmsse1[2, 6], "latex", bold = T)

crps1[6, 3] <- cell_spec(crps1[6, 3], "latex", bold = T)
crps1[6, 4] <- cell_spec(crps1[6, 4], "latex", bold = T)
crps1[6, 5] <- cell_spec(crps1[6, 5], "latex", bold = T)
crps1[7, 6] <- cell_spec(crps1[7, 6], "latex", bold = T)

kbl(mase1, booktabs = T, escape = F) |> 
  add_header_above(c(" " = 2, "MASE" = 4)) |> 
   kable_styling(latex_options = c("hold_position"))

kbl(rmsse1,booktabs = T, escape = F) %>%
  add_header_above(c(" " = 2, "RMSSE" = 4)) |> 
  kable_styling(latex_options = c("hold_position"))

kbl(crps1,booktabs = T, escape = F) %>%
  add_header_above(c(" " = 2, "CRPS" = 4)) |> 
 kable_styling(latex_options = c("hold_position"))
```

@tbl-result-2 presents the accuracy of the forecast distributions measures by CRPS, which considers both forecasting reliability and interval sharpness. The smaller the value of CRPS, the better the comprehensive performance. We observe that forecast reconciliation results in forecast improvement, regardless of the hierarchical level. Ensemble method is also more accurate fore higher levels, but ETS performs better at the bottom level. While results show that forecast reconciliation does not improve point forecast at the bottom level, however @tbl-result-2 indicates that it generates more accurate forecasts than base method. The outperformance of probabilistic forecast reconciliation at the bottom level may highlight an important fact. The reconciliation method may improve forecast accuracy at certain number of quantiles at the tails of forecast distribution, which are critical for managing risks.

Overall, our results indicate that forecast reconciliation using MinT method provides reliable forecasts, and all methods that reconcile using information at all levels of the forecast improve upon base (unreconciled) forecasts, except the bottom level series. For bottom series with less obvious patterns, base forecasts near center of forecast distribution might be more accurate, while forecast reconciliation using Mint could improve accuracy in the tails of of the distribution.

In addition to the overall forecast accuracy presented in @tbl-result, we also report the point and probabilistic forecast accuracy measures for each forecast horizon in @fig-accuracy. The figure focuses on the hierarchical levels important for decision making including total, control area and health board, however this could be calculated for any level. We only illustrate the results of the Mint method, given the its outperformance described in @tbl-result. For the illustration purpose, we report the average weekly forecast accuracy instead of the daily forecast horizon (i.e. 84 days including freezed planning (7 weeks = 42 days) plus future planing period (7 weeks = 42 days). Therefore, x-axis shows horizons from week 1 (h= 1,...,7) to week 12 (h= 78,...,84). The forecast horizon from week 7 to week 12 corresponds to the upcoming planning horizon, which is used by planners and decision making. For the point forecast, we can see that ensemble model performs much better than others regardless of forecast horizons. For the probabilistic forecasts, the distinction between forecasting models is less obvious. It is important to highlight that, all forecasting models outperform the Naive empirical distribution that is used as a Benchmark for both point and probabilsitic forecasts.


```{r}
#| label: fig-accuracy
#| fig-width: 8
#| fig-height: 5
#| out.width: "100%"
#| fig-cap: "Average accuracy by week for 12 weeks. CRPS is relative to a naive ECDF. MASE and MSSE are relative to the corresponding values for the training set."
#| fig-pos: "H"
crps <- crps |>
  pivot_longer(ensemble:tscount, names_to = "model", values_to = "crps")

accuracy <- bind_rows(
  rmsse |> mutate(measure = "msse", accuracy = rmsse^2),
  mase |> mutate(measure = "mase", accuracy = mase),
  crps |> mutate(measure = "crps", accuracy = crps),
) |>
  select(-rmsse, -mase, -crps)

# Plot of average accuracy vs week for each method for Total
acc_summary <- accuracy |>
  filter(
    series %in% c("Total", "Control areas", "Health boards"),
    method == "mint", model != "qcomb"
  ) |>
  mutate(
    series = factor(series, levels = c("Total", "Control areas", "Health boards")),
    model = factor(model, levels = c("naiveecdf", "ets", "tscount", "iglm", "ensemble")),
    week = factor(trunc((h - 1) / 7) + 1)
  ) |>
  group_by(week, model, measure, series) |>
  summarise(accuracy = mean(accuracy), .groups = "drop")

acc_summary |>
  ggplot(aes(x = week, y = accuracy, group = model, col = model)) +
  geom_line() +
  geom_point(size=.5)+
  facet_grid(measure ~ series, scales = "free_y") +
  labs(y = "Average accuracy", x = "Week ahead")+
  ggthemes::scale_color_colorblind() +
  ggthemes::theme_few()
```


