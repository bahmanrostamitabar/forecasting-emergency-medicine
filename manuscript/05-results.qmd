# Results and discussion {#sec-results}

```{r}
#| include: false
rmsse <- readr::read_rds(here::here("results/rmsse.rds")) |>
  mutate(msse = rmsse^2)
mase <- readr::read_rds(here::here("results/mase.rds"))
crps <- readr::read_rds(here::here("results/crps.rds")) |>
  pivot_wider(names_from = model, values_from = crps) |>
  group_by(method, h, series) |>
  mutate(across(where(is.numeric), ~ .x / naiveecdf)) |>
  ungroup()
```


In this section, we compare the forecasting performance of the Stationary, ETS, GLM, and TSGLM models along with the ensemble, using base forecast and Minimum Trace (MinT) reconciliation methods. We have also computed the forecast accuracy for Ordinary Least Square (OLS) and Weighted Least Square (WLS) approaches, along with bottom-up forecasting. However, they are not reported here because their accuracy is outperformed by MinT. We should also note that forecasts, and consequently their corresponding errors, are generated for the entire hierarchy and they could be reported at any level, if required. But to save space, we have reported only the top level (Total), the bottom level, and the levels corresponding to Control areas and Health boards. The latters are chosen because this is where decision-making takes place, so these forecasts are the most important.

The overall forecasting performance is reported in @tbl-result, in which the average forecast accuracy over horizons 43--84 days (corresponding to the planning horizon) is presented per model, method, and the hierarchical level. Reported forecast accuracy is averaged across all forecast horizons, rolling origins, and series at each level. @tbl-result presents both point and probabilistic forecast accuracy at total, control area, health board and bottom-level series. Point forecast performance is reported using MASE and MSSE, while probabilistic forecast accuracy is reported using CRPS. The bold entries in each table identify a combination of method and model that performs best for the corresponding level (i.e. each column), based on the smallest values of accuracy measures.

@tbl-result shows that forecast reconciliation (i.e. MinT) improves point forecast accuracy at the higher levels of the hierarchy including total, control area and health board. However, it does not result in accuracy improvement at the bottom-level series, for which base forecasts are more accurate. This might be due to the noisy structure of time series at the bottom level, and possibly due to very different patterns in the aggregated series. It is also clear from @tbl-result that the ensemble method improves forecast accuracy at total, control area and health board. However, this does not remain valid for bottom series where different individual methods perform best, depending on the accuracy measure. While the forecast reconciliation approach aims to enhance forecast accuracy, its effectiveness is not guaranteed, especially if the bottom-level series exhibit excessive noise and lack systematic patterns. Despite this, reconciling forecasts at the bottom level can offer advantages by generating coherent forecasts that facilitate alignment in planning across various teams within an organization, promote better coordination, and prevent conflicting decisions. Moreover, even when dealing with noisy and irregular bottom-level series, reconciliation can still improve forecast accuracy at higher levels of the hierarchy by leveraging the information available across the hierarchy. Therefore, although the bottom-level forecasts may not be highly accurate on their own, reconciling them with higher-level forecasts can still provide a more consistent view of future demand and potentially yield more accurate forecasts at other levels.

```{r}
#| label: tbl-result
#| tbl-cap: "Average forecast performance calculated on the test sets at forecast horizons $h=43,\\dots,84$ days, with time series cross validation applied to attended incident data. The test set consists of 462 days. The best approach is highlighted in bold. Point forecast accuracy is measured using MASE and MSSE, while probabilistic forecast accuracy is measured using CRPS."
# Set minimum in column to bold
set_to_bold <- function(table) {
  for (i in 3:6) {
    best <- which.min(table[[i]])
    table[[i]] <- sprintf(table[[i]], fmt = "%1.3f")
    table[[i]][best] <- paste0("\\textbf{", table[[i]][best], "}")
  }
  return(table)
}

msse1 <- rmsse |>
  filter(
    method %in% c("mint", "base"),
    model != "qcomb",
    h > 42,
    model != "ensemble2"
  ) |>
  mutate(
    model = factor(model,
      levels = c("naiveecdf", "ets", "iglm", "tscount", "ensemble", "ensemble2"),
      labels = c("Stationary", "ETS", "GLM", "TSGLM", "Ensemble", "Ensemble2")
    ),
    method = factor(method, levels = c("base", "mint"), labels = c("Base", "MinT")),
  ) |>
  group_by(method, model, series) |>
  summarise(msse = mean(msse), .groups = "drop") |>
  pivot_wider(names_from = series, values_from = msse) |>
  select(Method = method, Model = model, Total, `Control areas`, `Health boards`, Bottom) |>
  set_to_bold()
mase1 <- mase |>
  filter(
    method %in% c("mint", "base"),
    model != "qcomb",
    h > 42,
    model != "ensemble2"
  ) |>
  mutate(
    model = factor(model,
      levels = c("naiveecdf", "ets", "iglm", "tscount", "ensemble", "ensemble2"),
      labels = c("Stationary", "ETS", "GLM", "TSGLM", "Ensemble", "Ensemble2")
    ),
    method = factor(method, levels = c("base", "mint"), labels = c("Base", "MinT"))
  ) |>
  group_by(method, model, series) |>
  summarise(mase = mean(mase), .groups = "drop") |>
  pivot_wider(names_from = series, values_from = mase) |>
  select(Method = method, Model = model, Total, `Control areas`, `Health boards`, Bottom) |>
  set_to_bold()
crps1 <- readr::read_rds(here::here("results/crps.rds")) |>
  filter(
    method %in% c("mint", "base"),
    model != "qcomb",
    h > 42,
    model != "ensemble2"
  ) |>
  mutate(
    model = factor(model,
      levels = c("naiveecdf", "ets", "iglm", "tscount", "ensemble", "ensemble2"),
      labels = c("Stationary", "ETS", "GLM", "TSGLM", "Ensemble", "Ensemble2")
    ),
    method = factor(method, levels = c("base", "mint"), labels = c("Base", "MinT"))
  ) |>
  group_by(method, model, series) |>
  summarise(crps = mean(crps), .groups = "drop") |>
  pivot_wider(names_from = series, values_from = crps) |>
  select(Method = method, Model = model, Total, `Control areas`, `Health boards`, Bottom) |>
  set_to_bold()
bind_rows(mase1, msse1, crps1) |>
  kable(format = "latex", booktabs = TRUE, escape = FALSE, align = "llrrrr") |>
  pack_rows("MSSE", 1, 10) |>
  pack_rows("MASE", 11, 20, hline_before = TRUE) |>
  pack_rows("CRPS", 21, 30, hline_before = TRUE)
```

@tbl-result presents the accuracy of the forecast distributions measures by CRPS, which considers both forecasting reliability and interval sharpness. The smaller the value of CRPS, the better the comprehensive performance. We observe that forecast reconciliation results in forecast improvement for the total and health board level. CRPS is almost identical at the control area and botom levels. Base forecasts are slightly better at the control area level, while reconciliation is marginally accurate than base at the bottom level. The ensemble method is also more accurate for higher levels, but ETS performs well at the bottom level. @tbl-result also indicates that reconciliation using Mint  generates accurate *distributional* forecasts. The marginal improvement in the average probabilistic forecast accuracy at the bottom level might be due to the reconciliation method giving improved forecast accuracy in the tails of the forecast distribution, which are critical for managing risks.

Overall, our results indicate that forecast reconciliation using the MinT method provides reliable forecasts and improves upon the base (unreconciled) forecasts at all levels except the bottom-level series. But even there, forecast reconciliation using MinT improves accuracy in the tails of the distribution.


```{r}
#| label: fig-accuracy
#| fig-width: 8
#| fig-height: 5
#| out.width: "100%"
#| fig-cap: "Average accuracy by week for 12 weeks using MinT reconciliation. The total number of days used to calculate the accuracy in the test set is 462. Forecasts are generateled every 42 days, therefore we use 11 samples to calculate the average accuracy. CRPS is relative to a stationary Empirical Cumulative Distribution Function (ECDF). MASE and MSSE are relative to the corresponding values for the training set."
#| fig-pos: "H"
crps <- crps |>
  pivot_longer(ensemble:tscount, names_to = "model", values_to = "crps")

accuracy <- bind_rows(
  rmsse |> mutate(measure = "msse", accuracy = rmsse^2),
  mase |> mutate(measure = "mase", accuracy = mase),
  crps |> mutate(measure = "crps", accuracy = crps),
) |>
  select(-rmsse, -mase, -crps)

# Plot of average accuracy vs week for each method for Total
acc_summary <- accuracy |>
  filter(
    series %in% c("Total", "Control areas", "Health boards"),
    method == "mint", model != "qcomb"
  ) |>
  mutate(model = case_when(
    model == "naiveecdf" ~ "Stationary",
    model == "ets" ~ "ETS",
    model == "tscount" ~ "TSGLM",
    model == "iglm" ~ "GLM",
    model == "ensemble" ~ "Ensemble",
    model == "ensemble2" ~ "Ensemble2"
  )) |>
  mutate(
    measure = factor(measure, levels = c("mase", "msse", "crps"), labels = c("MASE", "MSSE", "CRPS")),
    series = factor(series, levels = c("Total", "Control areas", "Health boards")),
    model = factor(model, levels = c("Stationary", "ETS", "TSGLM", "GLM", "Ensemble", "Ensemble2")),
    week = factor(trunc((h - 1) / 7) + 1)
  ) |>
  group_by(week, model, measure, series) |>
  summarise(accuracy = mean(accuracy), .groups = "drop")


acc_summary |>
  filter(
    model != "Ensemble2"
  ) |>
  ggplot(aes(x = week, y = accuracy, group = model, col = model)) +
  geom_line() +
  geom_point(size = .5) +
  facet_grid(measure ~ series, scales = "free_y") +
  labs(y = "Average accuracy", x = "Week ahead") +
  ggthemes::scale_color_colorblind() +
  ggthemes::theme_few()
```

In addition to the overall forecast accuracy presented in @tbl-result, we also report the point and probabilistic forecast accuracy measures for each forecast horizon in @fig-accuracy. The figure focuses on the hierarchical levels important for decision-making including total, control area, and health board; however, the accuracy could be calculated for any level. We only illustrate the results of the MinT method, given its strong performance described in @tbl-result. For illustration purposes, we report the average weekly forecast accuracy instead of the daily forecast horizon, as this reduces the visual noise in the figure. Thus, the x-axis shows horizons from week 1 ($h= 1,\dots,7$) to week 12 ($h= 78,\dots,84$). The forecast horizon from week 7 to week 12 corresponds to the upcoming planning horizon, which is used by planners and decision-makers. For both the point forecast and distributional accuracy  we can see that the ensemble approach performs best across almost all horizons, with the biggest differences at the highest levels of aggregation. It is important to highlight that, all forecasting models outperform the stationary empirical distribution that is used as a benchmark for both point and probabilistic forecasts.

Despite using Poisson regression models to create count distributions of attended incidents for the base forecasts, it is important to note that the reconciled forecast distributions do not maintain a count format. In practical scenarios, there might be a need to use integer forecasts. Count forecast reconciliation is an active area of research, and it would be interesting to explore how our approach could be adapted to generate count-reconciled probabilistic forecasts in future studies. Rounding the forecasts is one possible solution to this problem. However, the impact of rounding on forecast accuracy varies depending on the level of hierarchy and the scale of the data. In situations with high volume demand, the effects of rounding may be negligible, and forecast accuracy calculations can overlook integer effects. On the other hand, in low-volume demand settings, such as forecasts at the bottom level of the hierarchy, integer (rounding) effects may have a more noticeable influence on forecast accuracy.


## An illustration of probabilistic forecast for EMS demand


In this section, we provide an illustrative example of a probabilistic forecast for future demand, of the total attended incidents in the SB health board. Due to the complexity of including such plots in the manuscript for the entire hierarchy and 84 days ahead, only one example is presented here. However, it is feasible to generate these plots for the entire hierarchy and for any forecast horizon if necessary.}

In practice, point forecasts are commonly used, but they have limitations as they ignore the uncertainty associated with the forecast. The future is inherently characterized by an irreducible level of uncertainty. Being prepared entails considering alternate courses of action. Probabilistic forecasts offer an alternative approach to anticipate future demand. Rather than providing a single value, they assign likelihoods to all possible demand outcomes, acknowledging that different numbers of attended incidents are possible, but with varying likelihoods.

The purpose of probabilistic forecasting, as demonstrated in @fig-forecast-density-hstep and @fig-forecast-density, is to quantify uncertainty. @fig-forecast-density-hstep depicts the forecast distribution of total incidents in one health board over a 7-day period. It also gives the point forecast as well as the 80\% and 90\% prediction intervals. @fig-forecast-density zooms in on the first day to show the histogram more clearly, illustrating the range of possible outcomes and their likelihood.

Decisions based on these forecasts could focus on the tails of the distribution: unexpectedly high demand leading to crowding and inefficiency, or unexpectedly low demand resulting in wasted resources. Such forecasts are valuable tools for decision-makers and planners, especially when dealing with low-probability, high-cost situations. Different EMS managements may have varying risk attitudes depending on resource availability, making it crucial to consider the entire distribution when making decisions. For instance, these forecasts enable management to calculate the probability of demand exceeding a certain threshold of available resources (e.g., 90\%), which can serve as an informative early warning measure for overcrowding.

It is important to note that while point forecasts and prediction intervals can be obtained from the probabilistic forecasts, the reverse is not possible. A single number cannot be used to directly derive a probabilistic forecast. Prediction intervals, although helpful in indicating possible ranges, do not provide information on the probabilities of low or high demand.

In EMS planning, future demand is just one aspect to consider. Other inputs, such as capacity, should also be treated as probability distributions to adopt a probabilistic approach to planning. To extract valuable insights and make informed decisions from probabilistic forecasts, specialized numerical tools are required, as the forecasts themselves are typically represented as explicit probability density functions or Monte Carlo generators.


```{r}
#| label: fig-forecast-density-hstep
#| out.width: "80%"
#| fig-cap: "A graphical illustration of the forecast distribution of ambulance demand (i.e. total incidence attended) for the SB health board for a horizon of seven days. For each day, we display the point forecast (black point), the histogram, and 80% (thick line) and 90% (thin line) prediction intervals. It also shows a portion of a historical time series as well as its fitted values."
#| fig-pos: "H"
incident_gthf <- readr::read_rds(here::here("data/incidents_gt.rds"))

incident_cv <- incident_gthf |> filter(lhb == "SB", is_aggregated(category), is_aggregated(nature))
fit <- incident_cv %>%
  model(
    ets = ETS(sqrt(incident))
  )
set.seed(2023)
fcst <- fit %>% generate(h = 7, times = 5000, bootstrap = TRUE)

fitted_ets <- fit |> augment()
fcst_point <- fcst |> index_by(date) |> summarise(.mean=mean(.sim))
p1 <- ggplot(data = fcst, mapping = aes(x = date, y = .sim))+
  stat_histinterval(point_interval = "mean_qi",.width = c(0.80, 0.95),breaks=15)+
  geom_line(data=fcst_point,aes(y=.mean, shape ="Point Forecast"))+
  geom_line(aes(y = .fitted, shape ="Fitted"), data = filter_index(fitted_ets, "2019-07-17" ~ .))+
  geom_point(aes(y = .fitted, shape ="Fitted"), data = filter_index(fitted_ets, "2019-07-17" ~ .))+
  geom_line(aes(y = incident, shape ="Actual"),data = filter_index(incident_cv, "2019-07-17" ~ .))+
  geom_point(aes(y = incident, shape ="Actual"),data = filter_index(incident_cv, "2019-07-17" ~ .))+
  scale_shape_manual(name=NULL,
                   breaks=c('Actual','Fitted','Point Forecast'),
                   values=c('Actual'=17,'Fitted'=15, 'Point Forecast'=19))+
  labs(x= "Date", y="Incidents")+
  theme_few()+
  theme(legend.position = "top")
p1
```

```{r}
#| label: fig-forecast-density
#| out.width: 80%
#| fig-cap: "An illustrative example of the forecast distribution of ambulance demand (i.e. total incidence attended) for the SB health board for one day ahead. This corresponds to the first forecast distribution in Figure 5. The horizontal axis shows all possible outcomes that may occur, with their likelihood shown on the vertical axis. The point in the middle shows the point forecast. Two lines at the bottom of the distribution highlights 80% (thick line) and 90% (thin line) prediction intervals."
#| fig-pos: "H"

fcst1 <- fcst %>% filter_index("2019-08-01")

p2 <- ggplot(data = fcst1, mapping = aes(x = .sim))+
  stat_histinterval(point_interval = "mean_qi",.width = c(0.80, 0.95),breaks=40)+
  scale_x_continuous(breaks = seq(60, 140, 5))+
  scale_y_continuous(labels = percent)+
  labs(y = "Density", x = "Incidents")+
  theme_few()
p2
```

