# Results and discussion {#sec-results}

```{r}
#| include: false
rmsse <- readr::read_rds(here::here("results/rmsse.rds")) |> 
  mutate(msse = rmsse^2)
mase <- readr::read_rds(here::here("results/mase.rds"))
crps <- readr::read_rds(here::here("results/crps.rds")) |>
  pivot_wider(names_from = model, values_from = crps) |>
  group_by(method, h, series) |>
  mutate(across(where(is.numeric), ~ .x / naiveecdf)) |>
  ungroup()
```


In this section, we compare the forecasting performance of the Naive, ETS, GLM, and TSGLM models along with the ensemble, using base forecast and Minimum Trace (MinT) reconciliation methods. We have also computed the forecast accuracy for Ordinary Least Square (OLS) and Weighted Least Square (WLS) approaches, along with bottom-up forecasting. However, they are not reported here because their accuracy is outperformed by MinT. We should also note that forecasts, and consequently their corresponding errors, are generated for the entire hierarchy and they could be reported at any level, if required. But to save space, we have reported only the top level (Total), the bottom level, and the levels corresponding to Control areas and Health boards. The latters are chosen because this is where decision-making takes place, so these forecasts are the most important.

The overall forecasting performance is reported in @tbl-result, in which the average forecast accuracy over horizons 43--84 days (corresponding to the planning horizon) is presented per model, method, and the hierarchical level. Reported forecast accuracy is averaged across all forecast horizons, rolling origins, and series at each level. @tbl-result presents both point and probabilistic forecast accuracy at total, control area, health board and bottom level series. Point forecast performance is reported using MASE and MSSE in @tbl-result-1  and [-@tbl-result-2], respectively. Probabilistic forecast accuracy is reported using CRPS in @tbl-result-3. The bold entries in each table identify a combination of method and model that performs best for the corresponding level (i.e. each column), based on the smallest values of accuracy measures.

@tbl-result-1 and [-@tbl-result-2] show that forecast reconciliation (i.e. MinT) improves forecast accuracy at the higher levels of the hierarchy including total, control area and health board. However, it is does not result in accuracy improvement at the bottom level series, for which base forecasts are more accurate. This might be due to the noisy structure of time series at the bottom level, and possibly due to very different patterns in the aggregated series. It is also clear from @tbl-result-1 that the ensemble method improves forecast accuracy at total, control area and health board. However, this does not remain valid for bottom series where different individual methods perform best, depending on the accuracy measure.

```{r}
#| label: tbl-result
#| tbl-cap: "Average forecast performance calculated on the test sets at forecast horizons $h=43,\\dots,84$ days, with time series cross validation applied to attended incident data. The best approach is highlighted in bold."
#| tbl-subcap:
#|   - "Point forecast accuracy using MASE"
#|   - "Point forecast accuracy using MSSE"
#|   - "Probabilistic forecast accuracy using CRPS"
#| layout-nrow: 3
#Set minimum in column to bold
set_to_bold <- function(table) {
  for(i in 3:6) {
    best <- which.min(table[[i]])
    table[[i]] <- sprintf(table[[i]], fmt="%1.3f")
    table[[i]][best] <- cell_spec(table[[i]][best], bold=TRUE)
  }
  return(table)
}

msse1 <- rmsse |>
  filter(
    method %in% c("mint", "base"),
    model != "qcomb",
    h > 42
  ) |>
  mutate(model = factor(model,
    levels = c("naiveecdf","ets","iglm","tscount","ensemble"),
    labels = c("Naive", "ETS", "GLM", "TSGLM","Ensemble")),
    method = factor(method, levels = c("base", "mint"), labels = c("Base", "MinT")),
  ) |>
  group_by(method, model, series) |>
  summarise(msse = mean(msse), .groups = "drop") |>
  pivot_wider(names_from = series, values_from = msse) |>
  select(Method = method, Model = model, Total, `Control areas`, `Health boards`, Bottom) |>
  set_to_bold()
mase1 <- mase |>
  filter(
    method %in% c("mint", "base"),
    model != "qcomb",
    h > 42
  ) |>
  mutate(
    model = factor(model,
      levels = c("naiveecdf","ets","iglm","tscount","ensemble"),
      labels = c("Naive", "ETS", "GLM", "TSGLM","Ensemble")),
    method = factor(method, levels = c("base", "mint"), labels = c("Base", "MinT"))
  ) |>
  group_by(method, model, series) |>
  summarise(mase = mean(mase), .groups = "drop") |>
  pivot_wider(names_from = series, values_from = mase) |>
  select(Method = method, Model = model, Total, `Control areas`, `Health boards`, Bottom) |>
  set_to_bold()
crps1 <- readr::read_rds(here::here("results/crps.rds")) |>
  filter(method %in% c("mint", "base"),
         model != "qcomb",
         h > 42
  ) |>
  mutate(model = factor(model,
    levels = c("naiveecdf","ets","iglm","tscount","ensemble"),
    labels = c("Naive", "ETS", "GLM", "TSGLM","Ensemble")),
    method = factor(method, levels = c("base", "mint"), labels = c("Base", "MinT"))
  ) |>
  group_by(method, model, series) |>
  summarise(crps = mean(crps), .groups = "drop") |>
  pivot_wider(names_from = series, values_from = crps) |>
  select(Method = method, Model = model, Total, `Control areas`, `Health boards`, Bottom) |>
  set_to_bold()

kbl(mase1, booktabs = TRUE, escape = FALSE, align="llrrrr") |>
  add_header_above(c(" " = 2, "MASE" = 4)) |>
  kable_styling(latex_options = c("hold_position"))

kbl(msse1, booktabs = TRUE, escape = FALSE, align="llrrrr") |>
  add_header_above(c(" " = 2, "MSSE" = 4)) |>
  kable_styling(latex_options = c("hold_position"))

kbl(crps1, booktabs = TRUE, escape = FALSE, align="llrrrr") |>
  add_header_above(c(" " = 2, "CRPS" = 4)) |>
  kable_styling(latex_options = c("hold_position"))
```

@tbl-result-3 presents the accuracy of the forecast distributions measures by CRPS, which considers both forecasting reliability and interval sharpness. The smaller the value of CRPS, the better the comprehensive performance. We observe that forecast reconciliation results in forecast improvement for the total and health board level. CRPS is almost identical at the control area and health board levels. Base forecasts are slightly better at the control area level, while reconciliation is marginally accurate than base at the bottom level. The ensemble method is also more accurate for higher levels, but ETS performs well at the bottom level. @tbl-result-3 indicates that it generates accurate *distributional* forecasts. The marginal improvement in the average probabilistic forecast accuracy at the bottom level might be due to the reconciliation method giving improved forecast accuracy in the tails of the forecast distribution, which are critical for managing risks.

Overall, our results indicate that forecast reconciliation using the MinT method provides reliable forecasts and improves upon the base (unreconciled) forecasts at all levels except the bottom-level series. But even there, forecast reconciliation using MinT improves accuracy in the tails of the distribution.

In addition to the overall forecast accuracy presented in @tbl-result, we also report the point and probabilistic forecast accuracy measures for each forecast horizon in @fig-accuracy. The figure focuses on the hierarchical levels important for decision-making including total, control area, and health board; however, the accuracy could be calculated for any level. We only illustrate the results of the MinT method, given its strong performance described in @tbl-result. For illustration purposes, we report the average weekly forecast accuracy instead of the daily forecast horizon, as this reduces the visual noise in the figure. Thus, the x-axis shows horizons from week 1 ($h= 1,\dots,7$) to week 12 ($h= 78,\dots,84$). The forecast horizon from week 7 to week 12 corresponds to the upcoming planning horizon, which is used by planners and decision-makers. For both the point forecast and distributional accuracy  we can see that the ensemble approach performs best across almost all horizons, with the biggest differences at the highest levels of aggregation. It is important to highlight that, all forecasting models outperform the Naive empirical distribution that is used as a benchmark for both point and probabilistic forecasts.


```{r}
#| label: fig-accuracy
#| fig-width: 8
#| fig-height: 5
#| out.width: "100%"
#| fig-cap: "Average accuracy by week for 12 weeks using MinT reconciliation. CRPS is relative to a naive Empirical Cumulative Distribution Function (ECDF). MASE and MSSE are relative to the corresponding values for the training set."
#| fig-pos: "H"
crps <- crps |>
  pivot_longer(ensemble:tscount, names_to = "model", values_to = "crps")

accuracy <- bind_rows(
  rmsse |> mutate(measure = "msse", accuracy = rmsse^2),
  mase |> mutate(measure = "mase", accuracy = mase),
  crps |> mutate(measure = "crps", accuracy = crps),
) |>
  select(-rmsse, -mase, -crps)

# Plot of average accuracy vs week for each method for Total
acc_summary <- accuracy |>
  filter(
    series %in% c("Total", "Control areas", "Health boards"),
    method == "mint", model != "qcomb"
  ) |>
  mutate(model = case_when(
    model == "naiveecdf" ~ "Naive",
    model == "ets" ~  "ETS",
    model == "tscount" ~ "TSGLM",
    model == "iglm" ~ "GLM",
    model == "ensemble" ~ "Ensemble"
  )) |> 
  mutate(
    measure = factor(measure, levels = c("mase","msse","crps"), labels = c("MASE","MSSE","CRPS")),
    series = factor(series, levels = c("Total", "Control areas", "Health boards")),
    model = factor(model, levels = c("Naive", "ETS", "TSGLM", "GLM", "Ensemble")),
    week = factor(trunc((h - 1) / 7) + 1)
  ) |>
  group_by(week, model, measure, series) |>
  summarise(accuracy = mean(accuracy), .groups = "drop") 
  

acc_summary |>
  ggplot(aes(x = week, y = accuracy, group = model, col = model)) +
  geom_line() +
  geom_point(size = .5) +
  facet_grid(measure ~ series, scales = "free_y") +
  labs(y = "Average accuracy", x = "Week ahead") +
  ggthemes::scale_color_colorblind() +
  ggthemes::theme_few()
```
