# Experiment setup {#sec-experiment}

Planners in the ambulance service work with a planning horizon of 6 weeks. That is, planning is generally frozen for the next 42 days, so any forecasts will only affect plans for the time period beyond the next 42 days. Consequently, the forecast horizon in this study is $2 \times 42 = 84$ days ahead, with performance evaluation assessed based on the last 42 days and not the whole forecast period. The forecasts are produced for various training and test sets using time series cross-validation [@hyndman2021forecasting].

In the following section, we discuss the dataset, describe the forecasting methods used to generate base forecasts, and present the point and probabilistic accuracy measures.

## Data {#sec-data}

The dataset used in this study is from a major ambulance service in Great Britain. It contains information relating to the daily number of attended incidents from 1 October 2015 to 31 July 2019, disaggregated by nature of incidents, priority, the health board managing the service and the control area (or region). @fig-hierarchy depicts both the hierarchical and grouped structure of the data. @fig-hierarchy-1 illustrates the nested hierarchical structure based on control area and health board and @fig-hierarchy-2 shows the grouped structure by priority and the nature of incident.

```{r}
#| label: fig-hierarchy
#| cache: true
#| out.width: "60%"
#| fig-cap: "The hierarchical and grouped structure of attended incidents (ambulance demand)."
#| fig-subcap:
#| - "Hierarchical structure: Attended incidents in the whole country are disaggregated into 3 control areas and then into 7 different healthboards."
#| - "Grouped structure: Incidents could be grouped into priority (i.e. Red, Amber & Green) and the nature of attended incident (i.e. there are 35 different nature of incidents including chest pain, breathing problems, heart attack, stroke, and so on). The symbol * refers to the crossed attributes between hierarchical and grouped levels."
#| layout: [[50],[-2], [38]]

data <- data.frame(
  level1 = "Total",
  level2 = c(
    "Central & West", "Central & West", "Central & West",
    "North", "South & East", "South & East", "South & East"
  ),
  level3 = c("HD", "SB", "PO", "BC", "CV", "CT", "AB")
)
# transform it to a edge list!
edges_level1_2 <- data %>%
  select(level1, level2) %>%
  unique() %>%
  rename(from = level1, to = level2)
edges_level2_3 <- data %>%
  select(level2, level3) %>%
  unique() %>%
  rename(from = level2, to = level3)
edge_list <- rbind(edges_level1_2, edges_level2_3)

mygraph <- igraph::graph_from_data_frame(edge_list)
ggraph::ggraph(mygraph, layout = "dendrogram", circular = FALSE) +
  ggraph::geom_edge_diagonal() +
  ggraph::geom_node_point(color = "#dddddd", size = 10) +
  ggraph::geom_node_text(
    aes(label = c(
      "All country",
      "Central & West", "North", "South & East",
      "HD", "SB", "PO", "BC", "CV", "CT", "AB"
    ))
  ) +
  theme_void()

knitr::include_graphics(here::here("img/group.png"))
```

@tbl-hierarchy also displays the structure of data with the total number of series at each level. At the top level, we have the total attended incidents for the country. We can split these total attended incidents by control area, by health board, by priority or by nature of incident. There are 3 control areas breakdown by 7 local health boards. Attended incident data are categorized into 3 priority classes of red, amber, and green. There are also 35 different nature of incidents such as chest pain, stroke, breathing problem, etc. In total, across all levels of disaggregation, there are 1530 time series.

```{r}
#| label: tbl-hierarchy
#| cache: true
#| tbl-cap: "Number of time series in each level for the hierarchical & grouped structure of attended incidents"
agg_level <- tibble::tribble(
  ~Level, ~`Number of series`,
  "All country", 1,
  "Control", 3,
  "Health board", 7,
  "Priority", 3,
  "Priority * Control", 3*3,
  "Priority * Health board", 3*7,
  "Nature of incident", 35,
  "Nature of incident * Control", 3*35,
  "Nature of incident * Health board", 7*35,
  "Priority * Nature of incident", 3*35-1,
  "Control * Priority * Nature of incident", 306,
  "Control * Health board * Priority * Nature of incident (Bottom level)", 691,
  "Total", 1530
)
knitr::kable(agg_level, booktabs = TRUE, position = "left") %>%
  kable_classic(full_width = FALSE) |> footnote(general = "Due to certain combinations of the nature of incident with other variables, there is a lack of representation in the dataset. As a result, for example, instead of the calculation 3 * 35 = 105, it would be modified to 3 * 35-1 = 104.",threeparttable = TRUE)
```

Given the total number of time series, direct visual analysis is infeasible. Therefore, we first compute features of all 1530 time series [@m3pca] and display the strength of trend and weekly seasonality strength in @fig-feature. Each point represents one time series with the strength of trend in x-axis and the strength of seasonality in y-axis. Both measures are on a scale of [0,1].

\textcolor{teal}{In this paper, the strength of trend and seasonality were calculated using the "STL" (Seasonal and Trend decomposition using Loess) decomposition method, as described by} @mstl. \textcolor{teal}{STL is a widely used and flexible method for decomposing time series data into trend, seasonal, and remainder components. The decomposition of a time series  $y_t$ is written as $y_t = T_t + S_{t} + R_t$, where $T_t$ is the smoothed trend component, $S_t$ is the seasonal component and $R_t$ is a remainder component. The strength of trend is defined as:}
$$F_T = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(T_t+R_t)}\right)$$
\textcolor{teal}{For strongly trended data, the seasonally adjusted data should have much more variation than the remainder component. Therefore Var($R_t$)/Var($T_t+R_t$) should be relatively small. But for data with little or no trend, the two variances should be approximately the same.}

\textcolor{teal}{The strength of seasonality is defined similarly:}
$$F_S = \max\left(0, 1 - \frac{\text{Var}(R_t)}{\text{Var}(S_{t}+R_t)}\right).$$
\textcolor{teal}{series with seasonal strength  $F_S$, close to 0 exhibits almost no seasonality, while a series with strong seasonality will have $F_S$ close to 1 because Var($R_t$) will be much smaller than Var($S_t+R_t$).}

It is clear that there are some series showing strong trends and/or seasonality, corresponding to series at the higher levels of the hierarchy. The majority of series show low trend and seasonality. These are time series belonging to the bottom series, series related to the nature of incidents for a given control, health board and priority level. Bottom series are dominated by noise with little or no systematic patterns.

```{r}
#| label: fig-feature
#| cache: true
#| out.width: "70%"
#| fig.align: center
#| fig-cap: "The strength of the trend and weekly seasonality in the time series of attended incidents. The scatter plot shows a total of 1530 data points, with each point corresponding to a specific time series."
incident_gthf <- readr::read_rds(here::here("data/incidents_gt.rds"))
incident_gthf %>%
  features(incident, feat_stl) %>%
  ggplot(aes(x = trend_strength, y = seasonal_strength_week)) +
  geom_point(alpha = 0.25) +
  labs(x = "Strength of trend", y = "Strength of weekly seasonality")
```

In addition to displaying the trend and seasonality features, we also visualize a few time series at various levels of aggregation. @fig-dataviz2 reveals different information such as trend, seasonality, and noise. For example, some series depict seasonality and trend, whereas some other series report low volume of attended incidents and entropy, making them more volatile and difficult to forecast. At the level on nature of incidents combined with categories of other levels, there are many series that contain zeros with low counts. As such, the data set represents a diverse set of daily time series patterns.

```{r}
#| label: fig-dataviz2
#| cache: true
#| dependson: "fig-feature"
#| out.width: "100%"
#| fig-width: 8
#| fig-height: 10
#| fig-cap: "Daily time plot of attended incidents at various levels. X-axis shows the date of incidents, consisting of 1400 data points (days) and y-axis shows the number of attended incidents. The panels show data from the whole country (top panel), by control area, by health board, by priority level, and by nature of incident. Only four of the 35 nature of incident categories are shown to avoid too much overplotting. Each time plot . "
no_x_axis <- theme(
  axis.title.x = element_blank(),
  axis.text.x = element_blank(),
  axis.ticks.x = element_blank()
)
p_total <- incident_gthf %>%
  filter(is_aggregated(region) & is_aggregated(lhb) & is_aggregated(category) & is_aggregated(nature)) %>%
  autoplot(incident) +
  labs(x = "", y = "Incidents") +
  ggthemes::scale_color_colorblind() +
  ggthemes::theme_few() +
  no_x_axis

p_control <- incident_gthf %>%
  filter(!is_aggregated(region) & !is_aggregated(lhb) & is_aggregated(category) & is_aggregated(nature)) %>%
  as_tibble() %>%
  select(-nature, -category) %>%
  group_by(date, region) %>%
  summarise(incident = sum(incident), .groups = "drop") %>%
  ggplot(aes(x = date, y = incident, color = factor(region))) +
  geom_line() +
  labs(y = "Incidents", color = "Control")  +
  ggthemes::scale_color_colorblind() +
  ggthemes::theme_few() +
  no_x_axis

p_board <- incident_gthf %>%
  filter(!is_aggregated(region) & !is_aggregated(lhb) & is_aggregated(category) & is_aggregated(nature)) %>%
  as_tibble() %>%
  select(-nature, -category) %>%
  ggplot(aes(x = date, y = incident, color = factor(lhb))) +
  geom_line() +
  # facet_wrap(vars(factor(region)), scales = "free_y") +
  labs(y = "Incidents", color = "Health board") +
  ggthemes::scale_color_colorblind() +
  ggthemes::theme_few() +
  no_x_axis

p_priority <- incident_gthf %>%
  filter(is_aggregated(region) & is_aggregated(lhb) & !is_aggregated(category) & is_aggregated(nature)) %>%
  mutate(
    category = recode(category, RED = "Red", AMB = "Amber", GRE = "Green"),
    category = factor(category, levels = c("Red", "Amber", "Green"))
  ) |>
  as_tibble() %>%
  select(-nature, -region) %>%
  ggplot(aes(x = date, y = incident, color = factor(category))) +
  geom_line() +
  scale_color_manual(values = c(Red = "#ff3300", Amber = "#E69f00", Green = "#009e73")) +
  labs(y = "Incidents", color = "Priority") +
  ggthemes::theme_few() +
  no_x_axis

selected_nature <- c("CHESTPAIN", "STROKECVA", "BREATHING", "ABDOMINAL")
p_nature <- incident_gthf %>%
  filter(is_aggregated(region) & is_aggregated(lhb) & is_aggregated(category) & !is_aggregated(nature)) %>%
  as_tibble() %>%
  mutate(nature = as.character(nature)) %>%
  filter(nature %in% selected_nature) %>%
  group_by(date, nature, lhb) %>%
  summarise(incident = sum(incident), .groups = "drop") %>%
  ggplot(aes(x = date, y = incident, color = nature)) +
  geom_line() +
  labs(x = "Date", y = "Incidents", color = "Nature of incident")+
  ggthemes::scale_color_colorblind() +
  ggthemes::theme_few()

p_total /
  p_control /
  p_board /
  p_priority /
  p_nature
```

We consider several forecasting models that account for the diverse patterns of the time series across the entire hierarchy. In developing the forecasting models, the time series of holidays are also used in addition to the attended incidents. We use public holidays, school holidays and Christmas Day and New Year's Day as predictors of incident attended. These types of holidays will affect peoples' activities and may increase or decrease the number of attended incidents.

## Forecasting methods

Given the presence of various patterns in the past attended incidents, we consider three different forecasting models to generate the base forecasts. Once the base forecasts are produced, hierarchical and grouped time series methods are used to reconcile them across all levels. We briefly discuss forecasting models in the following sections, and the hierarchical forecasting methods are discussed in @sec-htc.

**Stationary:** We start with a simple forecasting approach, assuming that the future days will be similar to past days. We use the empirical distribution of the past daily attended incidents to create the forecast distribution of future attended incidents. \textcolor{teal}{We have chosen this "stationary" method as a benchmark due to its widespread usage and simplicity, making it easily understandable for users. Forecasts serve as inputs for various decision-making systems that frequently employ simulations, wherein it is common to utilize the empirical distribution of demand as a forecast. Additionally, the stationary method has shown surprisingly high accuracy. Hence, any forecasting approach that can offer superior results compared to the stationary method would validate its practical use, otherwise there is no necessity for employing more complex methods.}

**Exponential Smoothing State Space model (ETS):** ETS models [@hyndman2021forecasting] can combine trend, seasonality, and error components in a time series through various forms that can be additive, multiplicative or mixed. The trend component can be none ("N"), Additive ("A") or damped ("Ad"); the seasonality can be none ("N"), Additive ("A"), or multiplicative ("M"); and the error term can be additive ("A") or multiplicative ("M"). To forecast the attended incidents at each level, we use the `ets()` function in the `forecast` package [@Rforecast;@HK08] in R. To identify the best model for a given time series, the `ets` function uses the corrected Akaike’s Information Criterion (AICc).

\textcolor{teal}{In our study, we use an automated algorithm to determine the suitable configuration for the trend, seasonality, and error terms in each time series. Specifically, we utilize the `ETS()` function in the fable package of R, which employs Akaike's Information Criterion (AIC) to identify the optimal model for each time series. Given the large number of time series we work with (1530), it is impractical to manually select the appropriate form for each component in every time series. Consequently, the automated algorithm selects the best model based on the unique characteristics of each individual time series. As a result, a combination of additive or multiplicative forms for the components are employed, depending on the specific attributes of each time series.}

Despite the popularity and the relevance of automatic `ETS` in this study, it may produce forecast distributions that are non-integer and include negative values, although the number of attended incidents is always integer and non-negative. When using `ETS`, a time series transformation approach could be used to generate strictly positive forecasts, although forecast distributions will still be non-integer. An alternative is to use forecasting models that produce integer, non-negative forecasts. In the following section we present Generalized Linear Models (GLMs) and Poisson time series regression to produce count base forecasts.

**Generalized Linear Model (GLM):** GLMs are a family of models developed to extend the concept of linear regression models to non-Gaussian distributions [@Faraway2016]. They model the response variable as a particular member of the exponential family, with the mean being a transformation of a linear function of the predictors. One of the models that is frequently used in practice to generate count forecasts is Poisson regression.

\textcolor{teal}{Suppose the time series is denoted by $y_1,\dots,y_T$, then the Poisson GLM can be written as}
\begin{align*}
  y_t &\sim \text{Poisson}(\lambda_t) \\
  \text{where}\qquad
  \log(\lambda_t) &= \bm{x}_t'\bm{\beta},
\end{align*}
\textcolor{teal}{and $\bm{x}_t$ is a vector of covariates, $\beta$ is a vector of coefficients, and $\lambda_t$ is the mean of the Poisson distribution. In our model, these include cubic splines for the time trend, day-of-week dummy variables (from Monday to Sunday), Fourier terms to capture the yearly seasonality, dummy variables indicating public holidays (1 when is public holiday, 0 otherwise), school holidays (1 when is school holiday, 0 otherwise) and Christmas Day (1 when is Christmas Day, 0 otherwise) and New Year's Day (1 when is New Year's Day, 0 otherwise). The Fourier terms are as defined in} @hyndman2021forecasting [Section 7.4] \textcolor{teal}{This model takes account of weekly seasonality and annual seasonality. Monthly seasonality is exceedingly rare in time series data, and does not occur in ambulance demand. There is no reason, for example, for incidents to occur more at some times of the month than others.}

We fit a Poisson regression model using the function `glm()` from the *stats* package in R, with the argument `family = poisson` to specify that we wish to fit a Poisson regression model with a log link function.

**Poisson Regression using tscount (TSGLM):** We also consider another Poisson regression model that takes into account serial dependence. This model captures the short range serial dependence by including autoregressive terms, in addition to the same covariates that were used in the GLM model. To distinguish this from the previous GLM model, we will refer to this model as `TSGLM`.

\textcolor{teal}{The Poisson TSGLM is similar to GLM with an additional autoregressive component accounting for serial correlation. The term serial dependence refers to situations where the ambulance demand at a given time point exhibits correlation with the demand at previous time points, after accounting for the exogenous covariates. Put simply, past values of the time series influence the current values, and various techniques can be employed to model this influence. TSGLM can be written as}
\begin{align*}
  y_t &\sim \text{Poisson}(\lambda_t) \\
  \text{where}\qquad
  \log(\lambda_t) &= (\bm{y}_{t-k}' , \bm{x}_t')\bm{\beta},
\end{align*}
\textcolor{teal}{and ${y}_{t-k}$ is a vector of $k$ lagged values. The TSGLM model explicitly accounts for serial dependence by including lagged values (i.e. past values) of the ambulance demand in the model. This is important in EMS forecasting because it allows the model to capture patterns in the data that are dependent on the past values of the time series, which might not be captured via the predictor variables.}

\textcolor{teal}{We use the `tsglm()` function in the `tscount` package in R} [@JSSv082i05] \textcolor{teal}{to model the attended incidents. Again, the logarithmic link function is used to ensure that the mean of the Poisson distribution is always positive.}

\textcolor{teal}{Provided accidents occur independently, they will inherently follow a Poisson distribution} [@feller1991introduction, p156--158]. \textcolor{teal}{Hence, it is reasonable to assume a Poisson distribution in this context. To account for changes over time, we incorporate trend and seasonality covariates, as well as public holiday effects, allowing the mean of the Poisson distribution to vary. However, it is important to note that if there are additional factors influencing the mean of the Poisson distribution, which are not accounted for in our model, we might observe over-dispersion or under-dispersion in the data.}

**Ensemble method:** \textcolor{teal}{Finally, one effective strategy for improving forecast accuracy includes the simultaneous application of multiple forecasting methods on a given time series, followed by combining the forecasts, rather than relying on separate forecasts generated by each individual method} [@clemen1989combining]. \textcolor{teal}{In this paper, we use an ensemble method that combines the forecasts generated from the Stationary, ETS, GLM and TSGLM models using a simple average to form a mixture distribution} [@combinations].

\textcolor{teal}{To generate forecast probability distributions using above methods, we use a form of bootstrapping, described in} @panagiotelis2023probabilistic. \textcolor{teal}{This involves simulating 1000 future sample paths from each of the models, by bootstrapping the model residuals, taking into account the cross-sectional correlations between the different aggregated and disaggregated series. In this way, we can generate an empirical distribution of forecasts for each model. The ensemble forecast distribution is a simple mixture of these empirical distributions.}

\textcolor{teal}{It is important to emphasize that the aim of this study is not to provide an exhaustive compilation of forecasting models, or to promote a particular model class. Instead, we have developed a flexible framework that can accommodate any forecasting models. Our primary objective is to demonstrate its practicality and effectiveness in integrating base forecasts from any model and generating coherent forecasts within a hierarchical structure.}


## Performance evaluation

To evaluate the performance of the various forecasting approaches, we split the data into a series of ten training and test sets. We use a time series cross-validation approach [@hyndman2021forecasting], with a forecast horizon of 84 days, and each training set expanding in 42-day steps. The first training set uses all data up to 2018-04-25, and the first test set uses the 84 days beginning 2018-04-26. The second training set uses all data up to 2018-06-06, with the second test set using the following 84 days. The largest training set ends on 2019-05-09, with the test set ending on 2019-07-31. Model development and hyper-parameter tuning is performed using the training data and the errors are assess using the corresponding test set. While we compute forecast errors for the entire 12 weeks, we are most interested in the last 42 days of each test set, because that corresponds to how forecasts are generated for planning in practice. Forecasting performance is evaluated using both point and probabilistic error measures.

\textcolor{teal}{The error metrics provided below consider a forecasting horizon denoted by $j$, representing the number of time periods ahead we are predicting. In our study, this forecasting horizon ranges from 1 to 84 days, $j= 1,2,\dots, 84$.}

Point forecast accuracy is measured via the Mean Squared Scaled Error (MSSE) and the Mean Absolute Scaled Error (MASE). The Mean Absolute Scaled Error (MASE) [@HK06; @hyndman2021forecasting] is calculated as:
$$
  \text{MASE} = \text{mean}(|q_{j}|),
$$
where
$$
  q_{j} = \frac{ e_{j}}
 {\displaystyle\frac{1}{T-m}\sum_{t=m+1}^T |y_{t}-y_{t-m}|},
$$
and $e_{j}$ is the point forecast error for forecast horizon $j$, $m = 7$ (as we have daily seasonal series), $y_t$ is the observation for period $t$, and $T$ is the sample size (the number of observations used for training the forecasting model). The denominator is the mean absolute error of the seasonal naive method in the fitting sample of $T$ observations and is used to scale the error. Smaller MASE values suggest more accurate forecasts. Note that the measure is scale-independent, thus allowing us to average the results across series.

A related measure is MSSE [@hyndman2021forecasting;@makridakis2022m5], which uses squared errors rather than absolute errors:
$$
  \text{MSSE} = \text{mean}(q_{j}^2),
$$ {#eq-RMSSE}
where,
$$
  q^2_{j} = \frac{ e^2_{j}}
 {\displaystyle\frac{1}{T-m}\sum_{t=m+1}^T (y_{t}-y_{t-m})^2},
$$
Again, this is scale-independent, and smaller MSSE values suggest more accurate forecasts.

\textcolor{teal}{Using scale-independent measures, such as MASE and MSSE, enables more appropriate comparisons between time series at different levels and scales, as these measures are not influenced by the magnitude of the data. This is of particular importance in our study, as we work with time series at various levels of hierarchy, with varying scales, resulting in different magnitudes of error. By employing scale-independent measures, we can meaningfully assess the forecast accuracy across the entire hierarchy, ensuring a more robust comparison.}

To measure the forecast distribution accuracy, we calculate the Continuous Rank Probability Score [@gneiting2014probabilistic;@hyndman2021forecasting]. It rewards sharpness and penalizes miscalibration, so it measures overall performance of the forecast distribution.
$$
  \text{CRPS} = \text{mean}(p_j),
$$ {#eq-CRPS}
where
$$
  p_j = \int_{-\infty}^{\infty} \left(G_j(x) - F_j(x)\right)^2dx,
$$
where $G_j(x)$ is the forecasted probability distribution function for forecast horizon $j$, and $F_j(x)$ is the true probability distribution function for the same period.

\textcolor{teal}{Calibration refers to the statistical consistency between the distributional forecasts and the observations. It measures how well the predicted probabilities match the observations. On the other hand, sharpness refers to the concentration of the forecast distributions --- a sharp forecast distribution results in narrow prediction intervals, indicating high confidence in the forecast. A model is well-calibrated if the predicted probabilities match the distribution of the observations, and it is sharp if it is confident in its predictions. The CRPS rewards sharpness and calibration by assigning lower scores to forecasts with sharper distributions, and to forecasts that are well-calibrated. Thus, it is a metric that combines both sharpness and miscalibration into a single score, making it a useful tool for evaluating the performance of probabilistic forecasts.}

\textcolor{teal}{CRPS can be considered an average of all possible Winkler scores} [@winkler1972decision;@hyndman2021forecasting, Section 5.9] \textcolor{teal}{or percentile scores} [@hyndman2021forecasting, Section 5.9], \textcolor{teal}{and thus provides an evaluation of all possible prediction intervals or quantiles.} \textcolor{teal}{A specific prediction interval could be evaluated using a Winkler score. Certain situations may also require assessing accuracy for a particular quantile, such as lower (e.g 5\%) or higher (e.g. 95\%) quantiles. In such cases, a percentile score becomes useful in meeting this specific requirement.}
